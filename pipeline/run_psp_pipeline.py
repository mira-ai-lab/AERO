#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import argparse
import os, json, yaml, subprocess, time, requests, shutil
from datetime import datetime
import glob
import random
from utils.io import read_jsonl, write_jsonl
from utils.make_kto_data import convert_to_kto_format

os.environ['http_proxy'] = ''
os.environ['https_proxy'] = ''

parser = argparse.ArgumentParser()
parser.add_argument("--exp_name", type=str, default="default_exp")
parser.add_argument("--port", type=int, default=8001)
parser.add_argument("--gpus", type=str, default=None)
parser.add_argument("--config", type=str, default="config.yaml")
args = parser.parse_args()

EXP_NAME = args.exp_name
EXP_ROOT = os.path.join("experiments", EXP_NAME)
os.makedirs(EXP_ROOT, exist_ok=True)

print(f"Loading config from: {args.config}")
CFG = yaml.safe_load(open(args.config, 'r', encoding='utf-8'))
STATE_FILE = os.path.join(EXP_ROOT, "pipeline_state.json")
VLLM_PORT = args.port
LLAMA_FACTORY_DIR = CFG["default"]["llama_factory_dir"]

if args.gpus:
    TRAIN_GPUS = args.gpus
else:
    TRAIN_GPUS = CFG["default"]["kto_gpus"]

# [Change] ä½¿ç”¨ KTO æ¨¡æ¿
KTO_TRAIN_TEMPLATE_YAML = os.path.join(LLAMA_FACTORY_DIR, CFG["default"]["kto_train_template_yaml"])
MERGE_TEMPLATE_YAML = os.path.join(LLAMA_FACTORY_DIR, CFG["default"]["kto_merge_template_yaml"])

def load_state():
    if os.path.exists(STATE_FILE):
        return json.load(open(STATE_FILE))
    else:
        return {"round": 0, "current_model": CFG["default"]["initial_model"], "history": []}

def save_state(state):
    json.dump(state, open(STATE_FILE, "w"), indent=2)

def restart_vllm_service(model_path: str, port: int):
    print(f"\n[vLLM] ğŸ”„ Deploying: {model_path}")
    subprocess.run(f"pkill -f 'vllm.*--port {port}' || true", shell=True)
    time.sleep(2)
    
    cmd = (f"CUDA_VISIBLE_DEVICES={TRAIN_GPUS} nohup vllm serve {model_path} "
           f"--port {port} --max-model-len 15360 --tensor-parallel-size 1 "
           f"--gpu-memory-utilization 0.9 --served-model-name psp_model " 
           f"> vllm_{EXP_NAME}.log 2>&1 &")
    subprocess.run(cmd, shell=True)
    
    health_url = f"http://localhost:{port}/health"
    print(f"[vLLM] Waiting for service...")
    for i in range(100):
        try:
            if requests.get(health_url, timeout=3).status_code == 200:
                print(f"[vLLM] âœ… Ready.")
                return
        except:
            pass
        time.sleep(5)
    raise RuntimeError("vLLM failed to start.")

def stop_vllm_service(port: int):
    subprocess.run(f"pkill -f 'vllm.*--port {port}' || true", shell=True)
    time.sleep(5)

# -------------------- [æ–°å¢å‡½æ•°: æƒé‡åº”ç”¨å’Œæ•°æ®å¤åˆ¶] --------------------
def apply_weights_and_replicate(kto_data: list, weights: dict) -> list:
    weighted_data = []
    
    default_weight = 1.0 

    for item in kto_data:
        data_type = item.get("type", "unknown")
        # æ ¹æ®é…ç½®è·å–æƒé‡
        weight = weights.get(data_type, default_weight)
        
        # æƒé‡å°äºç­‰äº 0 åˆ™å¿½ç•¥è¯¥æ•°æ®ç‚¹
        if weight <= 0:
            continue
            
        # 1. ä¿è¯çš„å¤åˆ¶æ¬¡æ•° (æ•´æ•°éƒ¨åˆ†)
        base_copies = int(weight)
        
        # 2. é¢å¤–å¤åˆ¶çš„æ¦‚ç‡ (å°æ•°éƒ¨åˆ†)
        extra_copy_prob = weight - base_copies 
        
        num_copies = base_copies
        if random.random() < extra_copy_prob:
            num_copies += 1
            
        # ç¡®ä¿åŸå§‹æ•°æ®è‡³å°‘è¢«ä¿ç•™ä¸€æ¬¡ï¼ˆå¦‚æœ weight ä»‹äº 0 åˆ° 1 ä¹‹é—´ï¼‰
        if num_copies == 0 and weight > 0:
             num_copies = 1 
             
        # å¤åˆ¶æ•°æ®
        for _ in range(num_copies):
            weighted_data.append(item)
            
    print(f"[KTO Data] Applied weights. Total samples after replication: {len(weighted_data)}")
    return weighted_data

# -------------------- [æ•°æ®å›æ”¾èšåˆ - ä¿æŒä¸å˜ï¼Œä½†å¢åŠ è¯»å–å½“å‰è½®æ¬¡æ•°æ®çš„å¥å£®æ€§] --------------------
def aggregate_kto_for_replay(exp_root, current_round_idx, replay_pool_size, replay_ratios): 
    """
    èšåˆæ‰€æœ‰å†å²è½®æ¬¡çš„ KTO æ•°æ®ï¼Œæ ¹æ® replay_ratios è¿›è¡Œæ¯”ä¾‹é‡‡æ ·ï¼Œå¹¶ä¸å½“å‰è½®æ¬¡æ•°æ®åˆå¹¶ã€‚
    """
    all_historical_data = []
    
    # 1. æ”¶é›†æ‰€æœ‰å†å²è½®æ¬¡ (1 åˆ° current_round_idx - 1) çš„ KTO æ•°æ®
    for r in range(1, current_round_idx): 
        kto_path = os.path.join(exp_root, f"outputs/round_{r}", "kto_data.jsonl")
        if os.path.exists(kto_path):
            try:
                data = read_jsonl(kto_path)
                all_historical_data.extend(data)
            except Exception as e:
                print(f"Error reading historical KTO data from {kto_path}: {e}")

    # 2. æŒ‰ç±»å‹åˆ†ç»„å†å²æ•°æ®
    historical_groups = {}
    for item in all_historical_data:
        data_type = item.get("type", "unknown")
        if data_type not in historical_groups:
            historical_groups[data_type] = []
        historical_groups[data_type].append(item)
        
    print(f"[KTO Data] Historical data collected: {len(all_historical_data)}. Grouped by type: {[f'{k}:{len(v)}' for k, v in historical_groups.items()]}")

    # 3. æ ¹æ®æ¯”ä¾‹è¿›è¡Œé‡‡æ ·
    replay_data = []
    total_historical_samples = 0
    sampled_counts = {}

    # å½’ä¸€åŒ–æ¯”ä¾‹
    total_ratio = sum(replay_ratios.values())
    if total_ratio == 0:
        print("[KTO Data] Warning: Total replay ratios sum to zero. No historical data will be replayed.")
    
    for data_type, ratio in replay_ratios.items():
        if data_type in historical_groups and total_ratio > 0:
            pool = historical_groups[data_type]
            normalized_ratio = ratio / total_ratio
            target_count = int(replay_pool_size * normalized_ratio)
            actual_count = min(len(pool), target_count)
            
            if actual_count > 0:
                random.shuffle(pool)
                replay_data.extend(pool[:actual_count])
                total_historical_samples += actual_count
                sampled_counts[data_type] = actual_count
                
    print(f"[KTO Data] Sampled historical data: {total_historical_samples}. Details: {sampled_counts}")
    
    # 4. æ”¶é›†å½“å‰è½®æ¬¡çš„æ•°æ®
    current_kto_path = os.path.join(exp_root, f"outputs/round_{current_round_idx}", "kto_data.jsonl")
    # ç¡®ä¿å½“å‰è½®æ¬¡çš„æ–‡ä»¶ä¸å­˜åœ¨æ—¶è¿”å›ç©ºåˆ—è¡¨
    current_data = read_jsonl(current_kto_path) if os.path.exists(current_kto_path) else [] 
    
    # 5. åˆå¹¶æ–°æ•°æ®å’Œå›æ”¾æ•°æ®
    final_dataset = current_data + replay_data
    
    print(f"[KTO Data] Total samples before weighting in Round {current_round_idx}: {len(final_dataset)} (New: {len(current_data)}, Replay: {total_historical_samples})")
    
    return final_dataset

# pipeline/run_psp_pipeline.py

def get_data_by_type_from_round(exp_root, round_idx, target_types):
    """
    ä»æŒ‡å®šè½®æ¬¡è¯»å–æ•°æ®ï¼Œå¹¶æ ¹æ® type è¿‡æ»¤ã€‚
    """
    kto_path = os.path.join(exp_root, f"outputs/round_{round_idx}", "kto_data.jsonl")
    if not os.path.exists(kto_path):
        return []
    
    data = read_jsonl(kto_path)
    filtered = [d for d in data if d.get("type") in target_types]
    return filtered

def aggregate_staggered_data(exp_root, current_round_idx, config):
    """
    å®ç°ä½ çš„æœºåˆ¶ï¼š
    - Question Data: æ¥è‡ª Current Round (N)
    - Solver Data: æ¥è‡ª Previous Round (N-1)
    """
    combined_data = []
    
    # å®šä¹‰å“ªäº› type å±äº Generatorï¼Œå“ªäº›å±äº Solver
    gen_types = ["question_generation", "question_generation_consistent", "question_generation_chaotic"]
    solver_types = ["answer_solver", "answer_refiner"]

    # 1. è·å–å½“å‰è½® (N) çš„ Generator æ•°æ®
    # è¿™éƒ¨åˆ†æ•°æ®åæ˜ äº†æ¨¡å‹åœ¨å½“å‰èƒ½åŠ›ä¸‹å¯¹é¢˜ç›®éš¾åº¦çš„æ¢ç´¢
    current_gen_data = get_data_by_type_from_round(exp_root, current_round_idx, gen_types)
    combined_data.extend(current_gen_data)
    print(f"[Staggered] Loaded {len(current_gen_data)} Question-Gen samples from Round {current_round_idx}")

    # 2. è·å–ä¸Šä¸€è½® (N-1) çš„ Solver æ•°æ®
    # å¦‚æœæ˜¯ç¬¬ 1 è½®ï¼ŒN-1=0ï¼Œé€šå¸¸æ²¡æœ‰è¾“å‡ºæ•°æ®ï¼Œæ‰€ä»¥è¿™ä¸€æ­¥ä¼šè·³è¿‡ï¼Œç¬¦åˆâ€œç¬¬0è½®åªè®­ç»ƒæé—®èƒ½åŠ›â€
    if current_round_idx > 1:
        prev_round = current_round_idx - 1
        prev_solver_data = get_data_by_type_from_round(exp_root, prev_round, solver_types)
        combined_data.extend(prev_solver_data)
        print(f"[Staggered] Loaded {len(prev_solver_data)} Solver samples from Round {prev_round}")
    else:
        print("[Staggered] Round 1: Skipping solver training (Solver data lag mechanism).")

    # 3. (å¯é€‰) ä¾ç„¶å¯ä»¥ä¿ç•™ Replay æœºåˆ¶ï¼Œä½†è¦å°å¿ƒä¸è¦å¼•å…¥ N è½®çš„ Solver æ•°æ®
    # å¦‚æœéœ€è¦ Replayï¼Œå»ºè®®åª Replay N-2 åŠä¹‹å‰çš„ Solver æ•°æ®
    
    return combined_data

def run_inner_loop(current_model, round_idx):
    print(f"[Round {round_idx}] ğŸš€ Inner Loop (Model: {current_model})")
    out_dir = os.path.join(EXP_ROOT, f"outputs/round_{round_idx}")
    os.makedirs(out_dir, exist_ok=True)
    
    marker = os.path.join(out_dir, "inner_logs.jsonl")
    kto_data_dir = os.path.join(EXP_ROOT, "kto_data")
    os.makedirs(kto_data_dir, exist_ok=True)

    if not os.path.exists(marker):
        env = os.environ.copy()
        env["CURRENT_MODEL"] = current_model
        cmd = [
            "python3", "-m", "synth.inner_loop",
            "--out_dir", out_dir,
            "--n_questions", str(CFG["default"]["questions_per_round"]),
            "--model_spec", current_model,
            "--round", str(round_idx),
            "--config", args.config,
            "--workers", "20"
        ]
        subprocess.run(cmd, check=True, env=env)
        
    else:
        print("[Skipping generation, data exists]")

    replay_pool_size = CFG["default"].get("replay_pool_size", 500)
    replay_ratios = CFG["default"].get("kto_replay_ratios", {
        "answer_solver": 0.5, 
        "answer_refiner": 0.3, 
        "question_generation": 0.2
    })

    # # è·å–åŸå§‹æ•°æ® (æ–°æ•°æ® + å›æ”¾æ•°æ®ï¼ŒæŒ‰æ¯”ä¾‹é‡‡æ ·)
    # raw_kto_dataset = aggregate_kto_for_replay(EXP_ROOT, round_idx, replay_pool_size, replay_ratios)
    
    # # [ä¿®å¤] 3. åº”ç”¨æƒé‡å¹¶è¿›è¡Œæ•°æ®å¤åˆ¶ (Replication)
    # # è¯»å–æƒé‡é…ç½®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™é»˜è®¤ä¸ºç©ºå­—å…¸ï¼ˆå³æƒé‡ä¸º 1.0ï¼‰
    # kto_weights = CFG["default"].get("kto_weights", {})
    # final_kto_dataset = apply_weights_and_replicate(raw_kto_dataset, kto_weights)

    # # 4. å†™å…¥èšåˆ/åŠ æƒåçš„æ•°æ®
    # master_kto_path = os.path.join(kto_data_dir, "kto_data.jsonl")
    # write_jsonl(master_kto_path, final_kto_dataset)

    # # Convert to KTO format
    # convert_to_kto_format(
    #     master_kto_path, 
    #     os.path.join(kto_data_dir, "kto_final.json")
    # )
    staggered_dataset = aggregate_staggered_data(EXP_ROOT, round_idx, CFG)
    
    # åº”ç”¨æƒé‡ (apply_weights_and_replicate éœ€è¦ç¡®ä¿èƒ½å¤„ç†)
    kto_weights = CFG["default"].get("kto_weights", {})
    final_kto_dataset = apply_weights_and_replicate(staggered_dataset, kto_weights)

    # å†™å…¥æ–‡ä»¶ï¼Œä¾› Outer Loop è¯»å–
    master_kto_path = os.path.join(kto_data_dir, "kto_data.jsonl") # æ³¨æ„è¿™é‡Œè¦†ç›–äº†
    write_jsonl(master_kto_path, final_kto_dataset)

    # Convert to KTO format
    convert_to_kto_format(
        master_kto_path, 
        os.path.join(kto_data_dir, "kto_final.json")
    )
def prepare_kto_data_for_llamafactory(round_idx, llama_factory_dir):
    dataset_name = f"{EXP_NAME}_kto_round_{round_idx}"
    file_name = f"{dataset_name}.json"
    
    # Copy file to LLaMA-Factory data dir
    src = os.path.join(EXP_ROOT, "kto_data", "kto_final.json")
    dst = os.path.join(llama_factory_dir, "data", file_name)
    shutil.copy(src, dst)
    
    # Update dataset_info.json
    info_path = os.path.join(llama_factory_dir, "data", "dataset_info.json")
    try:
        with open(info_path, 'r') as f: info = json.load(f)
    except: info = {}
    
    info[dataset_name] = {
        "file_name": file_name,
        "formatting": "sharegpt",  
        "columns": {
            "messages": "messages",
            "kto_tag": "label"    
        },
        "tags": {
            "role_tag": "role",        
            "content_tag": "content",   
            "user_tag": "user",         
            "assistant_tag": "assistant"
        }
    }
    
    with open(info_path, 'w') as f: json.dump(info, f, indent=4)
    return dataset_name

def run_outer_loop(base_model_path: str, round_idx: int):
    print(f"[Round {round_idx}] ğŸ§  KTO Training...")
    
    dataset_name = prepare_kto_data_for_llamafactory(round_idx, LLAMA_FACTORY_DIR)
    lora_output_dir = os.path.join(EXP_ROOT, f"saves/psp_round_{round_idx}")
    final_merged_dir = os.path.join(EXP_ROOT, f"models/psp_round_{round_idx}")
    
    train_yaml_path = os.path.join(EXP_ROOT, f"outputs/round_{round_idx}/kto_config.yaml")
    
    # Load & Modify Train Config
    with open(KTO_TRAIN_TEMPLATE_YAML, 'r') as f:
        cfg = yaml.safe_load(f)
    
    cfg["model_name_or_path"] = base_model_path
    cfg["dataset"] = dataset_name
    cfg["output_dir"] = lora_output_dir
    # ç¡®ä¿æ˜¯ KTO stage
    cfg["stage"] = "kto" 
    
    cfg["dataset_dir"] = "LLaMA-Factory/data"
    with open(train_yaml_path, 'w') as f: yaml.dump(cfg, f)
        
    # Train
    cmd_train = f"FORCE_TORCHRUN=1 CUDA_VISIBLE_DEVICES={TRAIN_GPUS} llamafactory-cli train {train_yaml_path}"
    subprocess.run(cmd_train, shell=True, check=True)
    
    # Merge
    print(f"[Round {round_idx}] ğŸ”„ Merging...")
    merge_yaml_path = os.path.join(EXP_ROOT, f"outputs/round_{round_idx}/merge_config.yaml")
    with open(MERGE_TEMPLATE_YAML, 'r') as f:
        mcfg = yaml.safe_load(f)
    mcfg["model_name_or_path"] = base_model_path
    mcfg["adapter_name_or_path"] = lora_output_dir
    mcfg["export_dir"] = final_merged_dir
    
    with open(merge_yaml_path, 'w') as f: yaml.dump(mcfg, f)
    
    subprocess.run(f"CUDA_VISIBLE_DEVICES={TRAIN_GPUS} llamafactory-cli export {merge_yaml_path}", shell=True, check=True)
    
    # Cleanup LoRA
    if os.path.exists(lora_output_dir): shutil.rmtree(lora_output_dir)
    return f"local::{final_merged_dir}"

def main():
    print(f"ğŸ”µ PSP Pipeline (Self-Play KTO) | Exp: {EXP_NAME}")
    state = load_state()
    current_model_path = ""
    
    if state["round"] == 0:
        init_path = CFG["default"]["init_model_path"]
        restart_vllm_service(init_path, VLLM_PORT)
        state["current_model"] = f"http::http://localhost:{VLLM_PORT}"
        current_model_path = init_path
        state["history"].append({"round":0, "model": f"local::{init_path}"})
        save_state(state)
    else:
        last = state["history"][-1]
        current_model_path = last["model"].replace("local::", "")
        restart_vllm_service(current_model_path, VLLM_PORT)

    for r in range(state["round"] + 1, CFG["default"]["rounds"] + 1):
        # 1. Inner Loop
        run_inner_loop(state["current_model"], r)
        
        # 2. Stop vLLM
        stop_vllm_service(VLLM_PORT)
        
        # 3. Outer Loop (KTO)
        new_model_local = run_outer_loop(current_model_path, r)
        
        # 4. Restart vLLM
        new_path = new_model_local.replace("local::", "")
        restart_vllm_service(new_path, VLLM_PORT)
        current_model_path = new_path
        
        # Update State
        state["round"] = r
        state["current_model"] = f"http::http://localhost:{VLLM_PORT}"
        state["history"].append({"round": r, "model": new_model_local})
        save_state(state)
        
    print("ğŸ¯ Pipeline Finished.")
    stop_vllm_service(VLLM_PORT)

if __name__ == "__main__":
    main()