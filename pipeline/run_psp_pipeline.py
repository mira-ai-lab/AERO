#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Physics Self-Play (PSP) pipeline
å¤šè½®è‡ªåšå¼ˆï¼šå†…å¾ªç¯ (æ•°æ®åˆæˆ + æ‰¹è¯„ + ç²¾ç‚¼) â†’ å¤–å¾ªç¯ (DPO è®­ç»ƒ)
æ¯è½®è®­ç»ƒå®Œæ¯•åè‡ªåŠ¨é‡æ–°éƒ¨ç½² vLLM åŠ è½½æ–°æ¨¡å‹ã€‚
"""

import os, json, yaml, subprocess, time, requests, shutil
from datetime import datetime
from cluster.cluster_agent import ClusterAgent
# å¯¼å…¥æˆ‘ä»¬éœ€è¦çš„ utils.io ä¸­çš„å‡½æ•°
from utils.io import read_jsonl

# ===== é…ç½®åŠ è½½ =====
CFG = yaml.safe_load(open("config.yaml"))
STATE_FILE = "pipeline/pipeline_state.json"
VLLM_PORT = 8001
# LLaMA-Factory ç›¸å…³é…ç½®
LLAMA_FACTORY_DIR = CFG["default"]["llama_factory_dir"]
DPO_GPUS = CFG["default"]["dpo_gpus"]
DPO_TRAIN_TEMPLATE_YAML = os.path.join(LLAMA_FACTORY_DIR, CFG["default"]["dpo_train_template_yaml"])
DPO_MERGE_TEMPLATE_YAML = os.path.join(LLAMA_FACTORY_DIR, CFG["default"]["dpo_merge_template_yaml"])


# ===== çŠ¶æ€ç®¡ç† =====
def load_state():
    if os.path.exists(STATE_FILE):
        return json.load(open(STATE_FILE))
    else:
        return {
            "round": 0,
            "current_model": CFG["default"]["initial_model"],
            "history": []
        }

def save_state(state):
    json.dump(state, open(STATE_FILE, "w", encoding="utf-8"),
              indent=2, ensure_ascii=False)

# ===== vLLM éƒ¨ç½² =====
def restart_vllm_service(model_path: str, port: int = 8000):
    """
    é‡å¯ vLLM æœåŠ¡ï¼Œä½¿å…¶åŠ è½½æ–°çš„æ¨¡å‹ã€‚
    é»˜è®¤å‡è®¾ vLLM å‘½ä»¤å¯ç”¨ï¼švllm serve <model_path> --port <port>
    """
    print(f"\n[vLLM] ğŸ”„ å‡†å¤‡é‡æ–°éƒ¨ç½²æ¨¡å‹: {model_path}")
    # 1. åœæ­¢æ—§è¿›ç¨‹
    subprocess.run(f"pkill -f 'vllm.*--port {port}' || true", shell=True)

    # 2. å¯åŠ¨æ–°æ¨¡å‹
    vllm_gpus = "4,5,6,7" 
    tensor_parallel_size = 4
    cmd = (f"CUDA_VISIBLE_DEVICES={vllm_gpus} nohup vllm serve {model_path} "
           f"--port {port} --max-model-len 8192 --tensor-parallel-size {tensor_parallel_size} "
           f"--served-model-name psp_model " 
           f"> vllm_round.log 2>&1 &")
    subprocess.run(cmd, shell=True)
    print(f"[vLLM] å¯åŠ¨å‘½ä»¤ï¼š{cmd}")

    # 3. ç­‰å¾…å¯åŠ¨
    ready = False
    health_url = f"http://localhost:{port}/health"
    print(f"[vLLM] æ­£åœ¨ç­‰å¾…æœåŠ¡å¯åŠ¨ (GET {health_url})...")

    for i in range(40):
        try:
            # ä½¿ç”¨ GET è¯·æ±‚è®¿é—® vLLM çš„ /health ç«¯ç‚¹
            r = requests.get(health_url, timeout=3)
            if r.status_code == 200:
                ready = True
                break
            else:
                print(f"[vLLM] ... (çŠ¶æ€: {r.status_code})")
                time.sleep(3)
        except requests.exceptions.ConnectionError:
            print("[vLLM] ... (è¿æ¥è¢«æ‹’ç»ï¼ŒvLLM å°šæœªå¯åŠ¨)")
            time.sleep(3)
        except Exception as e:
            print(f"[vLLM] ... (å‘ç”Ÿé”™è¯¯: {e})")
            time.sleep(3)

    if ready:
        print(f"[vLLM] âœ… æ–°æ¨¡å‹å·²ä¸Šçº¿ï¼šhttp://localhost:{port}\n")
    else:
        print(f"[vLLM] âš ï¸ è¶…æ—¶ï¼šè¯·æ£€æŸ¥ vLLM æ˜¯å¦æ­£å¸¸å¯åŠ¨ (æŸ¥çœ‹ vllm_round.log)ã€‚\n")
        # [é‡è¦] æŠ›å‡ºå¼‚å¸¸ä»¥åœæ­¢æµæ°´çº¿
        raise RuntimeError("vLLM service failed to start.")

def stop_vllm_service(port: int = 8000):
    """
    æ˜¾å¼åœæ­¢ vLLM æœåŠ¡ä»¥é‡Šæ”¾ GPU å†…å­˜ã€‚
    """
    print(f"\n[vLLM] ğŸ›‘ åœæ­¢ vLLM æœåŠ¡ (Port: {port}) ä»¥é‡Šæ”¾ GPU èµ„æº...")
    # 1. åœæ­¢è¿›ç¨‹
    subprocess.run(f"pkill -f 'vllm.*--port {port}' || true", shell=True)
    # ç»™äºˆä¸€äº›æ—¶é—´ç¡®ä¿è¿›ç¨‹å®Œå…¨é€€å‡º
    time.sleep(5) 
    print(f"[vLLM] âœ… æœåŠ¡å·²åœæ­¢ã€‚")

# ===== å†…å¾ªç¯ =====
def run_inner_loop(current_model, round_idx):
    print(f"[Round {round_idx}] ğŸš€ å†…å¾ªç¯å¯åŠ¨ï¼ˆæ¨¡å‹ï¼š{current_model}ï¼‰")
    out_dir = f"outputs/round_{round_idx}"
    os.makedirs(out_dir, exist_ok=True)
    env = os.environ.copy()
    env["CURRENT_MODEL"] = current_model
    cmd = [
        "python3", "-m", "synth.inner_loop",
        "--out_dir", out_dir,
        "--n_questions", str(CFG["default"]["questions_per_round"]),
        "--model_spec", current_model,
        "--round", str(round_idx), # ä¼ é€’è½®æ¬¡ä¿¡æ¯ï¼Œç”¨äºwram up
        "--max_refine", str(CFG["default"]["max_refine"])
    ]
    subprocess.run(cmd, check=True, env=env)
    
    # (é‡è¦) è¿è¡Œæ–°ä¿®æ”¹çš„ make_dpo_pairs è„šæœ¬
    # å®ƒç°åœ¨åªè½¬æ¢æ•°æ®ä¸º .json æ ¼å¼ï¼Œä¸å†åˆå¹¶
    cmd_dpo_convert = ["python3", "utils/make_dpo_pairs.py"]
    if os.name == 'posix':
        # ç¡®ä¿ä½¿ç”¨ -m æ–¹å¼è¿è¡Œï¼Œä»¥å¤„ç†æ¨¡å—è·¯å¾„é—®é¢˜
        cmd_dpo_convert = ["python3", "-m", "utils.make_dpo_pairs"]
    subprocess.run(cmd_dpo_convert, check=True)
    
    print(f"[Round {round_idx}] âœ… å†…å¾ªç¯å®Œæˆã€‚\n")


# ===== DPO æ•°æ®é›†å‡†å¤‡ (LLaMA-Factory) =====
def prepare_dpo_data_for_llamafactory(round_idx, llama_factory_dir):
    """
    åŠ¨æ€åˆå¹¶ã€å†™å…¥å’Œæ³¨å†Œ LLaMA-Factory æ‰€éœ€çš„æ•°æ®é›†ã€‚
    """
    print(f"[Round {round_idx}] Preparing DPO data for LLaMA-Factory...")
    
    # 1. å®šä¹‰æ•°æ®é›†åç§°å’Œè·¯å¾„
    dataset_name = f"psp_dpo_round_{round_idx}"
    file_name = f"{dataset_name}.json"
    dataset_file_path = os.path.join(llama_factory_dir, "data", file_name)
    dataset_info_path = os.path.join(llama_factory_dir, "data", "dataset_info.json")
    
    # 2. åˆå¹¶å·²è½¬æ¢çš„ ShareGPT æ ¼å¼æ•°æ®
    combined_data = []
    if os.path.exists("dpo_data/answers_dpo.json"):
        with open("dpo_data/answers_dpo.json", 'r', encoding='utf-8') as f:
            combined_data.extend(json.load(f))
    if os.path.exists("dpo_data/questions_dpo.json"):
        with open("dpo_data/questions_dpo.json", 'r', encoding='utf-8') as f:
            combined_data.extend(json.load(f))
    # [æ–°å¢] Critic
    if os.path.exists("dpo_data/critic_dpo.json"):
        print(f"  - Loading Critic preference data...")
        with open("dpo_data/critic_dpo.json", 'r') as f: combined_data.extend(json.load(f))
            
    # 3. å°†åˆå¹¶åçš„æ•°æ®å†™å…¥ LLaMA-Factory/data ç›®å½•
    with open(dataset_file_path, 'w', encoding='utf-8') as f:
        json.dump(combined_data, f, ensure_ascii=False, indent=2)
    print(f"Wrote {len(combined_data)} pairs to {dataset_file_path}")

    # 4. åŠ¨æ€æ³¨å†Œæ•°æ®é›†åˆ° dataset_info.json
    try:
        with open(dataset_info_path, 'r', encoding='utf-8') as f:
            dataset_info = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        dataset_info = {}

    dataset_info[dataset_name] = {
        "file_name": file_name,
        "ranking": True,
        "formatting": "sharegpt",
        "columns": {
            "messages": "conversations",
            "chosen": "chosen",
            "rejected": "rejected"
        }
    }
    
    with open(dataset_info_path, 'w', encoding='utf-8') as f:
        json.dump(dataset_info, f, ensure_ascii=False, indent=4)
    print(f"Registered '{dataset_name}' in {dataset_info_path}")

    return dataset_name


# ===== å¤–å¾ªç¯ (DPO è®­ç»ƒ - LoRA æ–¹å¼) =====
def run_outer_loop(base_model_path: str, round_idx: int):
    """
    æ‰§è¡Œ LLaMA-Factory LoRA DPO è®­ç»ƒä¸åˆå¹¶ã€‚
    è®­ç»ƒå®Œæˆåï¼Œåˆ é™¤ LoRA é€‚é…å™¨å’Œæ£€æŸ¥ç‚¹ï¼Œåªä¿ç•™æœ€ç»ˆåˆå¹¶çš„æ¨¡å‹ã€‚
    """
    print(f"[Round {round_idx}] ğŸ§  å¤–å¾ªç¯ DPO (LoRA) è®­ç»ƒä¸­...")
    
    # ===== [è°ƒè¯•ä¿®æ”¹] =====
    # 1. (æ³¨é‡Šæ‰) å‡†å¤‡å’Œæ³¨å†ŒåŠ¨æ€æ•°æ®é›†
    print(f"[Round {round_idx}] Preparing DPO data for LLaMA-Factory...")
    dataset_name = prepare_dpo_data_for_llamafactory(round_idx, LLAMA_FACTORY_DIR)
    
    # # 1. (æ–°) ä½¿ç”¨ä½ å·²åœ¨ dataset_info.json ä¸­æ³¨å†Œçš„å›ºå®šæ•°æ®é›†åç§°
    # dataset_name = "debug_dpo_data" 
    # print(f"âš ï¸ [DEBUG] æ­£åœ¨ä½¿ç”¨å›ºå®šçš„æ•°æ®é›†: {dataset_name}")
    # =====================
    
    # 2. åŠ¨æ€é…ç½® DPO è®­ç»ƒ YAML
    lora_output_dir = f"saves/psp_round_{round_idx}" # LoRA é€‚é…å™¨ä¿å­˜è·¯å¾„
    dynamic_train_yaml_path = f"outputs/round_{round_idx}/dpo_train_config.yaml"
    with open(DPO_TRAIN_TEMPLATE_YAML, 'r', encoding='utf-8') as f:
        train_config = yaml.safe_load(f)
        
    train_config["model_name_or_path"] = base_model_path
    train_config["dataset"] = dataset_name
    train_config["output_dir"] = lora_output_dir

    train_config["dataset_dir"] = os.path.join(LLAMA_FACTORY_DIR, "data")
    
    with open(dynamic_train_yaml_path, 'w', encoding='utf-8') as f:
        yaml.dump(train_config, f)
        
    # 3. æ‰§è¡Œ DPO è®­ç»ƒå‘½ä»¤
    cmd_train = (f"FORCE_TORCHRUN=1 CUDA_VISIBLE_DEVICES={DPO_GPUS} "
                 f"llamafactory-cli train {dynamic_train_yaml_path}")
    print(f"[RUN] {cmd_train}")
    subprocess.run(cmd_train, shell=True, check=True)
    print(f"[Round {round_idx}] âœ… DPO è®­ç»ƒ (LoRA) å®Œæˆ. é€‚é…å™¨ä¿å­˜åœ¨ {lora_output_dir}")

    # 4. åŠ¨æ€é…ç½®æ¨¡å‹åˆå¹¶ YAML
    print(f"[Round {round_idx}] ğŸ”„ åˆå¹¶æ¨¡å‹ä¸­...")
    final_merged_model_dir = f"models/psp_round_{round_idx}" # æœ€ç»ˆå®Œæ•´æ¨¡å‹è·¯å¾„
    dynamic_merge_yaml_path = f"outputs/round_{round_idx}/merge_config.yaml"
    
    with open(DPO_MERGE_TEMPLATE_YAML, 'r', encoding='utf-8') as f:
        merge_config = yaml.safe_load(f)
        
    merge_config["model_name_or_path"] = base_model_path
    merge_config["adapter_name_or_path"] = lora_output_dir
    merge_config["export_dir"] = final_merged_model_dir

    with open(dynamic_merge_yaml_path, 'w', encoding='utf-8') as f:
        yaml.dump(merge_config, f)

    # 5. æ‰§è¡Œæ¨¡å‹åˆå¹¶å‘½ä»¤
    cmd_merge = f"CUDA_VISIBLE_DEVICES={DPO_GPUS} llamafactory-cli export {dynamic_merge_yaml_path}"
    print(f"[RUN] {cmd_merge}")
    subprocess.run(cmd_merge, shell=True, check=True)
    
    print(f"[Round {round_idx}] âœ… æ¨¡å‹åˆå¹¶å®Œæˆï¼Œæ–°æ¨¡å‹ä¿å­˜è‡³ {final_merged_model_dir}")

    # =====================================================
    # 11/18 æ¸…ç† LoRA æƒé‡å’Œæ£€æŸ¥ç‚¹
    # =====================================================
    if os.path.exists(lora_output_dir):
        print(f"[Cleanup] ğŸ—‘ï¸ æ­£åœ¨åˆ é™¤ LoRA ä¸­é—´äº§ç‰© (èŠ‚çœç©ºé—´): {lora_output_dir}")
        try:
            shutil.rmtree(lora_output_dir)
            print(f"[Cleanup] âœ… å·²åˆ é™¤ {lora_output_dir}")
        except Exception as e:
            print(f"[Cleanup] âš ï¸ åˆ é™¤å¤±è´¥: {e}")
    # =====================================================
    return f"local::{final_merged_model_dir}"


# ===== ClusterAgent =====
def cluster_and_update_prompt(round_idx):
    import json
    path = f"outputs/round_{round_idx}/inner_results.jsonl"
    if not os.path.exists(path):
        print("âš ï¸ æœªæ‰¾åˆ° inner_results.jsonlï¼Œè·³è¿‡ cluster åˆ†æã€‚")
        return
    questions = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                j = json.loads(line)
                questions.append(j.get("question",""))
    ca = ClusterAgent(n_clusters=CFG["default"]["cluster"]["n_clusters"])
    res = ca.analyze_and_suggest(
        questions,
        entropy_threshold=CFG["default"]["cluster"]["entropy_threshold"]
    )
    if res.get("suggestion"):
        print("[ClusterAgent] ğŸ”„ æ›´æ–°ç”Ÿæˆå™¨ promptï¼š", res["suggestion"]["prompt_suggestion"])
        ca.apply_suggestion_to_prompt(res["suggestion"], "synth/prompt_template.txt")
    else:
        print("[ClusterAgent] âœ… é—®é¢˜åˆ†å¸ƒè‰¯å¥½ï¼Œæ— éœ€ä¿®æ”¹ promptã€‚")
    with open(f"outputs/round_{round_idx}/cluster_report.json", "w", encoding="utf-8") as f:
        json.dump(res, f, ensure_ascii=False, indent=2)

# ===== ä¸»æµç¨‹ (ä¿®æ”¹) =====
def main():
    state = load_state()
    total_rounds = CFG["default"]["rounds"]
    
    current_model_path = "" # (æ–°) è·Ÿè¸ªå½“å‰æ¨¡å‹çš„ *æ–‡ä»¶è·¯å¾„*

    # é¦–æ¬¡è¿è¡Œæ—¶ï¼Œéƒ¨ç½²åˆå§‹æ¨¡å‹
    if state["round"] == 0:
        init_model_path = "/data/gaozhitao/modelhub/Qwen2.5-7B-Instruct" # (ç¡¬ç¼–ç çš„åˆå§‹æ¨¡å‹è·¯å¾„)
        restart_vllm_service(init_model_path, port=VLLM_PORT)
        state["current_model"] = f"http::http://localhost:{VLLM_PORT}"
        
        # (æ–°) å°†åˆå§‹æ¨¡å‹è·¯å¾„ä¿å­˜åˆ° history ä¸­ï¼Œä»¥ä¾¿ç¬¬ä¸€è½® DPO ä½¿ç”¨
        state["history"].append({
            "round": 0,
            "model": f"local::{init_model_path}", # ä¿å­˜åˆå§‹æ¨¡å‹çš„è·¯å¾„
            "timestamp": datetime.now().isoformat()
        })
        current_model_path = init_model_path # (æ–°) è®¾ç½®å½“å‰è·¯å¾„
        save_state(state)
    else:
        # (æ–°) å¦‚æœä¸æ˜¯é¦–æ¬¡è¿è¡Œï¼Œä» history åŠ è½½æœ€æ–°çš„æ¨¡å‹è·¯å¾„
        current_model_path = state["history"][-1]["model"].replace("local::", "")

    for r in range(state["round"] + 1, total_rounds + 1):
        cur_model_endpoint = state["current_model"] # (è¿™æ˜¯ vLLM çš„ http åœ°å€)
        print(f"\n===== ğŸŒ Round {r} å¯åŠ¨ (å½“å‰æ¨¡å‹: {cur_model_endpoint}) =====")
        print(f"æœ¬è½® DPO è®­ç»ƒå°†åŸºäºæ¨¡å‹è·¯å¾„: {current_model_path}")


        # å†…å¾ªç¯ (ä½¿ç”¨ vLLM endpoint)
        run_inner_loop(cur_model_endpoint, r)

        # èšç±»åˆ†æä¸ prompt æ›´æ–°
        # cluster_and_update_prompt(r)

        # ===== [æ–°æ­¥éª¤] åœæ­¢ vLLM ä»¥é‡Šæ”¾ GPU =====
        print(f"[Round {r}] é‡Šæ”¾ GPUï¼šå‡†å¤‡åœæ­¢ vLLM æœåŠ¡...")
        stop_vllm_service(port=VLLM_PORT)
        print(f"[Round {r}] GPU å·²é‡Šæ”¾ï¼Œå‡†å¤‡ DPO è®­ç»ƒ...")
        # =========================================

        # å¤–å¾ªç¯è®­ç»ƒ (ä½¿ç”¨ base_model_path)
        new_model_local = run_outer_loop(current_model_path, r)

        # é‡æ–°éƒ¨ç½² vLLM
        new_model_path = new_model_local.replace("local::", "")
        restart_vllm_service(new_model_path, port=VLLM_PORT)

        # (æ–°) æ›´æ–° current_model_path ä»¥ä¾¿ä¸‹ä¸€è½® DPO ä½¿ç”¨
        current_model_path = new_model_path 
        
        # æ›´æ–°çŠ¶æ€
        state["round"] = r
        state["current_model"] = f"http::http://localhost:{VLLM_PORT}"
        state["history"].append({
            "round": r,
            "model": new_model_local, # (è¿™é‡Œä¿å­˜çš„æ˜¯ local::/path/to/new/model)
            "timestamp": datetime.now().isoformat()
        })
        save_state(state)

        print(f"âœ… Round {r} å®Œæˆï¼ŒvLLM å·²æ›´æ–°ä¸ºæ–°æ¨¡å‹ã€‚")
        print("============================================\n")

    print("ğŸ¯ å…¨éƒ¨è½®æ¬¡ PSP è®­ç»ƒå®Œæˆã€‚")
    print(f"[Round {r}] é‡Šæ”¾ GPUï¼šå‡†å¤‡åœæ­¢ vLLM æœåŠ¡...")
    stop_vllm_service(port=VLLM_PORT)
    print(f"[Round {r}] GPU å·²é‡Šæ”¾ï¼Œå‡†å¤‡ DPO è®­ç»ƒ...")

if __name__ == "__main__":
    main()