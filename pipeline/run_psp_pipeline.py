#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Physics Self-Play (PSP) pipeline
å¤šè½®è‡ªåšå¼ˆï¼šå†…å¾ªç¯ (æ•°æ®åˆæˆ + æ‰¹è¯„ + ç²¾ç‚¼) â†’ å¤–å¾ªç¯ (DPO è®­ç»ƒ)
æ¯è½®è®­ç»ƒå®Œæ¯•åè‡ªåŠ¨é‡æ–°éƒ¨ç½² vLLM åŠ è½½æ–°æ¨¡å‹ã€‚
"""
import argparse
import os, json, yaml, subprocess, time, requests, shutil
from datetime import datetime
from cluster.cluster_agent import ClusterAgent
from utils.io import read_jsonl
from utils.make_dpo_pairs import convert_pairs_to_sharegpt

# ===== 1. è·å–å®éªŒåç§° =====
parser = argparse.ArgumentParser()
parser.add_argument("--exp_name", type=str, default="default_exp", help="å®éªŒåç§°ï¼Œç”¨äºéš”ç¦»æ•°æ®å’Œæ¨¡å‹")
parser.add_argument("--port", type=int, default=8001, help="vLLM æœåŠ¡ç«¯å£")
args = parser.parse_args()

EXP_NAME = args.exp_name
# æ‰€æœ‰è¯¥å®éªŒçš„æ•°æ®éƒ½æ”¾åœ¨ experiments/{EXP_NAME}/ ä¸‹
EXP_ROOT = os.path.join("experiments", EXP_NAME)
os.makedirs(EXP_ROOT, exist_ok=True)

# ===== é…ç½®åŠ è½½ =====
CFG = yaml.safe_load(open("config.yaml"))
STATE_FILE = "pipeline/pipeline_state.json"
VLLM_PORT = args.port
# LLaMA-Factory ç›¸å…³é…ç½®
LLAMA_FACTORY_DIR = CFG["default"]["llama_factory_dir"]
DPO_GPUS = CFG["default"]["dpo_gpus"]
DPO_TRAIN_TEMPLATE_YAML = os.path.join(LLAMA_FACTORY_DIR, CFG["default"]["dpo_train_template_yaml"])
DPO_MERGE_TEMPLATE_YAML = os.path.join(LLAMA_FACTORY_DIR, CFG["default"]["dpo_merge_template_yaml"])


# ===== çŠ¶æ€ç®¡ç† =====
def load_state():
    if os.path.exists(STATE_FILE):
        return json.load(open(STATE_FILE))
    else:
        return {
            "round": 0,
            "current_model": CFG["default"]["initial_model"],
            "history": []
        }

def save_state(state):
    json.dump(state, open(STATE_FILE, "w", encoding="utf-8"),
              indent=2, ensure_ascii=False)

# ===== vLLM éƒ¨ç½² =====
def restart_vllm_service(model_path: str, port: int = 8000):
    """
    é‡å¯ vLLM æœåŠ¡ï¼Œä½¿å…¶åŠ è½½æ–°çš„æ¨¡å‹ã€‚
    é»˜è®¤å‡è®¾ vLLM å‘½ä»¤å¯ç”¨ï¼švllm serve <model_path> --port <port>
    """
    print(f"\n[vLLM] ğŸ”„ å‡†å¤‡é‡æ–°éƒ¨ç½²æ¨¡å‹: {model_path}")
    # 1. åœæ­¢æ—§è¿›ç¨‹
    subprocess.run(f"pkill -f 'vllm.*--port {port}' || true", shell=True)

    # 2. å¯åŠ¨æ–°æ¨¡å‹
    vllm_gpus = "0,1" 
    tensor_parallel_size = 2
    cmd = (f"CUDA_VISIBLE_DEVICES={vllm_gpus} nohup vllm serve {model_path} "
           f"--port {port} --max-model-len 8192 --tensor-parallel-size {tensor_parallel_size} --gpu-memory-utilization 0.95 "
           f"--served-model-name psp_model " 
           f"> vllm_round.log 2>&1 &")
    subprocess.run(cmd, shell=True)
    print(f"[vLLM] å¯åŠ¨å‘½ä»¤ï¼š{cmd}")

    # 3. ç­‰å¾…å¯åŠ¨
    ready = False
    health_url = f"http://localhost:{port}/health"
    print(f"[vLLM] æ­£åœ¨ç­‰å¾…æœåŠ¡å¯åŠ¨ (GET {health_url})...")

    for i in range(100):
        try:
            # ä½¿ç”¨ GET è¯·æ±‚è®¿é—® vLLM çš„ /health ç«¯ç‚¹
            r = requests.get(health_url, timeout=3)
            if r.status_code == 200:
                ready = True
                break
            else:
                print(f"[vLLM] ... (çŠ¶æ€: {r.status_code})")
                time.sleep(5)
        except requests.exceptions.ConnectionError:
            print("[vLLM] ... (è¿æ¥è¢«æ‹’ç»ï¼ŒvLLM å°šæœªå¯åŠ¨)")
            time.sleep(3)
        except Exception as e:
            print(f"[vLLM] ... (å‘ç”Ÿé”™è¯¯: {e})")
            time.sleep(5)

    if ready:
        print(f"[vLLM] âœ… æ–°æ¨¡å‹å·²ä¸Šçº¿ï¼šhttp://localhost:{port}\n")
    else:
        print(f"[vLLM] âš ï¸ è¶…æ—¶ï¼šè¯·æ£€æŸ¥ vLLM æ˜¯å¦æ­£å¸¸å¯åŠ¨ (æŸ¥çœ‹ vllm_round.log)ã€‚\n")
        # [é‡è¦] æŠ›å‡ºå¼‚å¸¸ä»¥åœæ­¢æµæ°´çº¿
        raise RuntimeError("vLLM service failed to start.")

def stop_vllm_service(port: int = 8000):
    """
    æ˜¾å¼åœæ­¢ vLLM æœåŠ¡ä»¥é‡Šæ”¾ GPU å†…å­˜ã€‚
    """
    print(f"\n[vLLM] ğŸ›‘ åœæ­¢ vLLM æœåŠ¡ (Port: {port}) ä»¥é‡Šæ”¾ GPU èµ„æº...")
    # 1. åœæ­¢è¿›ç¨‹
    subprocess.run(f"pkill -f 'vllm.*--port {port}' || true", shell=True)
    # ç»™äºˆä¸€äº›æ—¶é—´ç¡®ä¿è¿›ç¨‹å®Œå…¨é€€å‡º
    time.sleep(5) 
    print(f"[vLLM] âœ… æœåŠ¡å·²åœæ­¢ã€‚")

# ===== å†…å¾ªç¯ =====
def run_inner_loop(current_model, round_idx):
    print(f"[Round {round_idx}] ğŸš€ å†…å¾ªç¯å¯åŠ¨ï¼ˆæ¨¡å‹ï¼š{current_model}ï¼‰")
    out_dir = os.path.join(EXP_ROOT, f"outputs/round_{round_idx}")
    os.makedirs(out_dir, exist_ok=True)

    # [æ–°å¢] æ£€æŸ¥ç‚¹é€»è¾‘ï¼šå¦‚æœç»“æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ™è·³è¿‡ç”Ÿæˆ
    marker_file = os.path.join(out_dir, "inner_results.jsonl")
    run_generation = True

    dpo_data_dir = os.path.join(EXP_ROOT, "dpo_data")
    os.makedirs(dpo_data_dir, exist_ok=True)
    
    if os.path.exists(marker_file):
        print(f"[Round {round_idx}] âš ï¸ æ£€æµ‹åˆ°å†…å¾ªç¯æ•°æ®å·²å­˜åœ¨: {marker_file}")
        print(f"[Round {round_idx}] â­ï¸ è·³è¿‡æ•°æ®ç”Ÿæˆé˜¶æ®µï¼Œç›´æ¥æ¢å¤æ•°æ®çŠ¶æ€...")
        run_generation = False
        
        # æ¢å¤æ•°æ®é€»è¾‘
        files_to_restore = ["answers_pairs.jsonl", "questions_pairs.jsonl", "critic_pairs.jsonl"]
        for fname in files_to_restore:
            src = os.path.join(out_dir, fname)
            dst = os.path.join(dpo_data_dir, fname)
            if os.path.exists(src):
                shutil.copy(src, dst)

    if run_generation:
        env = os.environ.copy()
        env["CURRENT_MODEL"] = current_model
        cmd = [
            "python3", "-m", "synth.inner_loop",
            "--out_dir", out_dir,
            "--n_questions", str(CFG["default"]["questions_per_round"]),
            "--model_spec", current_model,
            "--round", str(round_idx) # [æ–°å¢] ä¼ é€’è½®æ¬¡ä¿¡æ¯
        ]
        subprocess.run(cmd, check=True, env=env)

        files_to_copy = ["answers_pairs.jsonl", "questions_pairs.jsonl", "critic_pairs.jsonl"]
        for fname in files_to_copy:
            src = os.path.join(out_dir, fname)
            dst = os.path.join(dpo_data_dir, fname)
            if os.path.exists(src):
                shutil.copy(src, dst)
    
    print(f"[Round {round_idx}] Converting to ShareGPT format...")
    
    pairs_map = {
        "answers_pairs.jsonl": "answers_dpo.json",
        "questions_pairs.jsonl": "questions_dpo.json",
        "critic_pairs.jsonl": "critic_dpo.json"
    }
    
    for input_name, output_name in pairs_map.items():
        inp_path = os.path.join(dpo_data_dir, input_name)
        out_path = os.path.join(dpo_data_dir, output_name)
        if os.path.exists(inp_path):
            convert_pairs_to_sharegpt(inp_path, out_path)

    print(f"[Round {round_idx}] âœ… å†…å¾ªç¯å‡†å¤‡å®Œæˆã€‚\n")


# ===== DPO æ•°æ®é›†å‡†å¤‡ (LLaMA-Factory) =====
def prepare_dpo_data_for_llamafactory(round_idx, llama_factory_dir):
    dataset_name = f"{EXP_NAME}_dpo_round_{round_idx}"
    file_name = f"{dataset_name}.json"
    
    dataset_file_path = os.path.join(llama_factory_dir, "data", file_name)
    dataset_info_path = os.path.join(llama_factory_dir, "data", "dataset_info.json")
    
    dpo_data_dir = os.path.join(EXP_ROOT, "dpo_data")
    
    combined_data = []
    for fname in ["answers_dpo.json", "questions_dpo.json", "critic_dpo.json"]:
        fpath = os.path.join(dpo_data_dir, fname)
        if os.path.exists(fpath):
            with open(fpath, 'r', encoding='utf-8') as f:
                combined_data.extend(json.load(f))
            
    with open(dataset_file_path, 'w', encoding='utf-8') as f:
        json.dump(combined_data, f, ensure_ascii=False, indent=2)
    
    try:
        with open(dataset_info_path, 'r', encoding='utf-8') as f:
            dataset_info = json.load(f)
    except Exception:
        dataset_info = {}

    dataset_info[dataset_name] = {
        "file_name": file_name,
        "ranking": True,
        "formatting": "sharegpt",
        "columns": {"messages": "conversations", "chosen": "chosen", "rejected": "rejected"}
    }
    
    with open(dataset_info_path, 'w', encoding='utf-8') as f:
        json.dump(dataset_info, f, ensure_ascii=False, indent=4)
        
    return dataset_name


# ===== å¤–å¾ªç¯ (DPO è®­ç»ƒ - LoRA æ–¹å¼) =====
def run_outer_loop(base_model_path: str, round_idx: int):
    """
    æ‰§è¡Œ LLaMA-Factory LoRA DPO è®­ç»ƒä¸åˆå¹¶ã€‚
    è®­ç»ƒå®Œæˆåï¼Œåˆ é™¤ LoRA é€‚é…å™¨å’Œæ£€æŸ¥ç‚¹ï¼Œåªä¿ç•™æœ€ç»ˆåˆå¹¶çš„æ¨¡å‹ã€‚
    """
    dataset_name = prepare_dpo_data_for_llamafactory(round_idx, LLAMA_FACTORY_DIR)
    
    # [ä¿®æ”¹] LoRA å’Œ Merge è·¯å¾„åŸºäº EXP_ROOT
    lora_output_dir = os.path.join(EXP_ROOT, f"saves/psp_round_{round_idx}")
    final_merged_model_dir = os.path.join(EXP_ROOT, f"models/psp_round_{round_idx}")
    
    # Config æ–‡ä»¶ä¹Ÿä¿å­˜åˆ°å®éªŒç›®å½•
    dynamic_train_yaml_path = os.path.join(EXP_ROOT, f"outputs/round_{round_idx}/dpo_train_config.yaml")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(dynamic_train_yaml_path), exist_ok=True)

    with open(DPO_TRAIN_TEMPLATE_YAML, 'r', encoding='utf-8') as f:
        train_config = yaml.safe_load(f)
        
    train_config["model_name_or_path"] = base_model_path
    train_config["dataset"] = dataset_name
    train_config["output_dir"] = lora_output_dir
    train_config["dataset_dir"] = os.path.join(LLAMA_FACTORY_DIR, "data")
    
    with open(dynamic_train_yaml_path, 'w', encoding='utf-8') as f:
        yaml.dump(train_config, f)
        
    # 3. æ‰§è¡Œ DPO è®­ç»ƒå‘½ä»¤
    cmd_train = (f"FORCE_TORCHRUN=1 CUDA_VISIBLE_DEVICES={DPO_GPUS} "
                 f"llamafactory-cli train {dynamic_train_yaml_path}")
    print(f"[RUN] {cmd_train}")
    subprocess.run(cmd_train, shell=True, check=True)
    print(f"[Round {round_idx}] âœ… DPO è®­ç»ƒ (LoRA) å®Œæˆ. é€‚é…å™¨ä¿å­˜åœ¨ {lora_output_dir}")

    # 4. åŠ¨æ€é…ç½®æ¨¡å‹åˆå¹¶ YAML
    print(f"[Round {round_idx}] ğŸ”„ åˆå¹¶æ¨¡å‹ä¸­...")
    final_merged_model_dir = f"models/psp_round_{round_idx}" # æœ€ç»ˆå®Œæ•´æ¨¡å‹è·¯å¾„
    dynamic_merge_yaml_path = f"outputs/round_{round_idx}/merge_config.yaml"
    
    with open(DPO_MERGE_TEMPLATE_YAML, 'r', encoding='utf-8') as f:
        merge_config = yaml.safe_load(f)
        
    merge_config["model_name_or_path"] = base_model_path
    merge_config["adapter_name_or_path"] = lora_output_dir
    merge_config["export_dir"] = final_merged_model_dir

    with open(dynamic_merge_yaml_path, 'w', encoding='utf-8') as f:
        yaml.dump(merge_config, f)

    # 5. æ‰§è¡Œæ¨¡å‹åˆå¹¶å‘½ä»¤
    cmd_merge = f"CUDA_VISIBLE_DEVICES={DPO_GPUS} llamafactory-cli export {dynamic_merge_yaml_path}"
    print(f"[RUN] {cmd_merge}")
    subprocess.run(cmd_merge, shell=True, check=True)
    
    print(f"[Round {round_idx}] âœ… æ¨¡å‹åˆå¹¶å®Œæˆï¼Œæ–°æ¨¡å‹ä¿å­˜è‡³ {final_merged_model_dir}")

    # =====================================================
    # 11/18 æ¸…ç† LoRA æƒé‡å’Œæ£€æŸ¥ç‚¹
    # =====================================================
    if os.path.exists(lora_output_dir):
        print(f"[Cleanup] ğŸ—‘ï¸ æ­£åœ¨åˆ é™¤ LoRA ä¸­é—´äº§ç‰© (èŠ‚çœç©ºé—´): {lora_output_dir}")
        try:
            shutil.rmtree(lora_output_dir)
            print(f"[Cleanup] âœ… å·²åˆ é™¤ {lora_output_dir}")
        except Exception as e:
            print(f"[Cleanup] âš ï¸ åˆ é™¤å¤±è´¥: {e}")
    # =====================================================
    return f"local::{final_merged_model_dir}"


# ===== ClusterAgent =====
def cluster_and_update_prompt(round_idx):
    import json
    path = os.path.join(EXP_ROOT, f"outputs/round_{round_idx}/inner_results.jsonl")
    if not os.path.exists(path):
        print("âš ï¸ æœªæ‰¾åˆ° inner_results.jsonlï¼Œè·³è¿‡ cluster åˆ†æã€‚")
        return
    questions = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                j = json.loads(line)
                questions.append(j.get("question",""))
    ca = ClusterAgent(n_clusters=CFG["default"]["cluster"]["n_clusters"])
    res = ca.analyze_and_suggest(
        questions,
        entropy_threshold=CFG["default"]["cluster"]["entropy_threshold"]
    )
    if res.get("suggestion"):
        print("[ClusterAgent] ğŸ”„ æ›´æ–°ç”Ÿæˆå™¨ promptï¼š", res["suggestion"]["prompt_suggestion"])
        ca.apply_suggestion_to_prompt(res["suggestion"], "synth/prompt_template.txt")
    else:
        print("[ClusterAgent] âœ… é—®é¢˜åˆ†å¸ƒè‰¯å¥½ï¼Œæ— éœ€ä¿®æ”¹ promptã€‚")
    with open(f"outputs/round_{round_idx}/cluster_report.json", "w", encoding="utf-8") as f:
        json.dump(res, f, ensure_ascii=False, indent=2)

# ===== ä¸»æµç¨‹ (ä¿®æ”¹) =====
def main():
    print(f"ğŸ”µ å¯åŠ¨ PSP Pipeline | å®éªŒåç§°: {EXP_NAME}")
    print(f"ğŸ“‚ å®éªŒæ ¹ç›®å½•: {EXP_ROOT}")

    state = load_state()
    total_rounds = CFG["default"]["rounds"]
    
    current_model_path = "" # (æ–°) è·Ÿè¸ªå½“å‰æ¨¡å‹çš„ *æ–‡ä»¶è·¯å¾„*

    # é¦–æ¬¡è¿è¡Œæ—¶ï¼Œéƒ¨ç½²åˆå§‹æ¨¡å‹
    if state["round"] == 0:
        init_model_path = CFG["default"]["init_model_path"]
        restart_vllm_service(init_model_path, port=VLLM_PORT)
        state["current_model"] = f"http::http://localhost:{VLLM_PORT}"
        
        state["history"].append({
            "round": 0,
            "model": f"local::{init_model_path}",
            "timestamp": datetime.now().isoformat()
        })
        current_model_path = init_model_path 
        save_state(state)
    else:
        # === [ä¿®å¤] æ–­ç‚¹ç»­è®­é€»è¾‘ ===
        # å¦‚æœä¸æ˜¯é¦–æ¬¡è¿è¡Œï¼Œä» history åŠ è½½æœ€æ–°çš„æ¨¡å‹è·¯å¾„ï¼Œå¹¶é‡æ–°éƒ¨ç½² vLLM
        if not state["history"]:
            raise ValueError("State shows round > 0 but history is empty!")
            
        last_model_record = state["history"][-1]
        current_model_path = last_model_record["model"].replace("local::", "")
        
        print(f"âš ï¸ [Resume] æ£€æµ‹åˆ°ä¸­æ–­çŠ¶æ€ (Round {state['round']})ã€‚")
        print(f"ğŸ”„ æ­£åœ¨æ¢å¤éƒ¨ç½²ä¸Šä¸€è½®çš„æ¨¡å‹: {current_model_path}")
        
        # è¿™ä¸€æ­¥æ˜¯å…³é”®ï¼šå¿…é¡»åœ¨è¿›å…¥å¾ªç¯å‰æŠŠæœåŠ¡æ‹‰èµ·æ¥
        restart_vllm_service(current_model_path, port=VLLM_PORT)
        
        # ç¡®ä¿å†…å­˜ä¸­çš„ state URL æ˜¯æ­£ç¡®çš„
        state["current_model"] = f"http::http://localhost:{VLLM_PORT}"
        # ===========================

    for r in range(state["round"] + 1, total_rounds + 1):
        cur_model_endpoint = state["current_model"] 
        print(f"\n===== ğŸŒ Round {r} å¯åŠ¨ (å½“å‰æ¨¡å‹: {cur_model_endpoint}) =====")
        print(f"æœ¬è½® DPO è®­ç»ƒå°†åŸºäºæ¨¡å‹è·¯å¾„: {current_model_path}")

        # å†…å¾ªç¯ (ä½¿ç”¨ vLLM endpoint)
        run_inner_loop(cur_model_endpoint, r)

        # èšç±»åˆ†æä¸ prompt æ›´æ–°
        # cluster_and_update_prompt(r)

        # åœæ­¢ vLLM ä»¥é‡Šæ”¾ GPU
        print(f"[Round {r}] é‡Šæ”¾ GPUï¼šå‡†å¤‡åœæ­¢ vLLM æœåŠ¡...")
        stop_vllm_service(port=VLLM_PORT)
        print(f"[Round {r}] GPU å·²é‡Šæ”¾ï¼Œå‡†å¤‡ DPO è®­ç»ƒ...")

        # å¤–å¾ªç¯è®­ç»ƒ
        new_model_local = run_outer_loop(current_model_path, r)

        # é‡æ–°éƒ¨ç½² vLLM
        new_model_path = new_model_local.replace("local::", "")
        restart_vllm_service(new_model_path, port=VLLM_PORT)

        # æ›´æ–°è·¯å¾„
        current_model_path = new_model_path 
        
        # æ›´æ–°çŠ¶æ€
        state["round"] = r
        state["current_model"] = f"http::http://localhost:{VLLM_PORT}"
        state["history"].append({
            "round": r,
            "model": new_model_local,
            "timestamp": datetime.now().isoformat()
        })
        save_state(state)

        print(f"âœ… Round {r} å®Œæˆï¼ŒvLLM å·²æ›´æ–°ä¸ºæ–°æ¨¡å‹ã€‚")
        print("============================================\n")

    print("ğŸ¯ å…¨éƒ¨è½®æ¬¡ PSP è®­ç»ƒå®Œæˆã€‚")
    # è®­ç»ƒç»“æŸåä¹Ÿå¯ä»¥é€‰æ‹©åœæ­¢æœåŠ¡
    stop_vllm_service(port=VLLM_PORT)
if __name__ == "__main__":
    main()