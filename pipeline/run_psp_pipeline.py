#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Physics Self-Play (PSP) pipeline
å¤šè½®è‡ªåšå¼ˆï¼šå†…å¾ªç¯ (æ•°æ®åˆæˆ + æ‰¹è¯„ + ç²¾ç‚¼) â†’ å¤–å¾ªç¯ (DPO è®­ç»ƒ)
æ¯è½®è®­ç»ƒå®Œæ¯•åè‡ªåŠ¨é‡æ–°éƒ¨ç½² vLLM åŠ è½½æ–°æ¨¡å‹ã€‚
"""

import os, json, yaml, subprocess, time, requests
from datetime import datetime
from cluster.cluster_agent import ClusterAgent

# ===== é…ç½®åŠ è½½ =====
CFG = yaml.safe_load(open("config.yaml"))
STATE_FILE = "pipeline/pipeline_state.json"
VLLM_PORT = 8000

# ===== çŠ¶æ€ç®¡ç† =====
def load_state():
    if os.path.exists(STATE_FILE):
        return json.load(open(STATE_FILE))
    else:
        return {
            "round": 0,
            "current_model": CFG["default"]["initial_model"],
            "history": []
        }

def save_state(state):
    json.dump(state, open(STATE_FILE, "w", encoding="utf-8"),
              indent=2, ensure_ascii=False)

# ===== vLLM éƒ¨ç½² =====
def restart_vllm_service(model_path: str, port: int = 8000):
    """
    é‡å¯ vLLM æœåŠ¡ï¼Œä½¿å…¶åŠ è½½æ–°çš„æ¨¡å‹ã€‚
    é»˜è®¤å‡è®¾ vLLM å‘½ä»¤å¯ç”¨ï¼švllm serve <model_path> --port <port>
    """
    print(f"\n[vLLM] ğŸ”„ å‡†å¤‡é‡æ–°éƒ¨ç½²æ¨¡å‹: {model_path}")
    # 1. åœæ­¢æ—§è¿›ç¨‹
    subprocess.run(f"pkill -f 'vllm.*--port {port}' || true", shell=True)

    # 2. å¯åŠ¨æ–°æ¨¡å‹
    cmd = f"nohup vllm serve {model_path} --port {port} --max-model-len 8192 > vllm_round.log 2>&1 &"
    subprocess.run(cmd, shell=True)
    print(f"[vLLM] å¯åŠ¨å‘½ä»¤ï¼š{cmd}")

    # 3. ç­‰å¾…å¯åŠ¨
    ready = False
    for i in range(40):
        try:
            r = requests.post(f"http://localhost:{port}/generate",
                              json={"prompt": "ping"}, timeout=2)
            if r.status_code == 200:
                ready = True
                break
        except Exception:
            time.sleep(3)
    if ready:
        print(f"[vLLM] âœ… æ–°æ¨¡å‹å·²ä¸Šçº¿ï¼šhttp://localhost:{port}/generate\n")
    else:
        print(f"[vLLM] âš ï¸ è¶…æ—¶ï¼šè¯·æ£€æŸ¥ vLLM æ˜¯å¦æ­£å¸¸å¯åŠ¨ã€‚\n")

# ===== å†…å¾ªç¯ =====
def run_inner_loop(current_model, round_idx):
    print(f"[Round {round_idx}] ğŸš€ å†…å¾ªç¯å¯åŠ¨ï¼ˆæ¨¡å‹ï¼š{current_model}ï¼‰")
    out_dir = f"outputs/round_{round_idx}"
    os.makedirs(out_dir, exist_ok=True)
    env = os.environ.copy()
    env["CURRENT_MODEL"] = current_model
    cmd = [
        "python3", "synth/inner_loop.py",
        "--out_dir", out_dir,
        "--n_questions", str(CFG["default"]["questions_per_round"]),
        "--model_spec", current_model
    ]
    subprocess.run(cmd, check=True, env=env)
    subprocess.run(["python3", "dpo/make_dpo_pairs.py"], check=True)
    print(f"[Round {round_idx}] âœ… å†…å¾ªç¯å®Œæˆã€‚\n")

# ===== å¤–å¾ªç¯ (DPO è®­ç»ƒ) =====
def run_outer_loop(current_model, round_idx):
    print(f"[Round {round_idx}] ğŸ§  å¤–å¾ªç¯ DPO è®­ç»ƒä¸­...")
    out_dir = f"models/psp_round_{round_idx}"
    os.makedirs(out_dir, exist_ok=True)
    cmd_template = CFG["default"]["dpo_train_cmd_template"]
    cmd = cmd_template.format(model=current_model, out_dir=out_dir)
    print(f"[RUN] {cmd}")
    subprocess.run(cmd, shell=True, check=True)
    print(f"[Round {round_idx}] âœ… å¤–å¾ªç¯è®­ç»ƒå®Œæˆï¼Œæ–°æ¨¡å‹ä¿å­˜è‡³ {out_dir}")
    return f"local::{out_dir}"

# ===== ClusterAgent =====
def cluster_and_update_prompt(round_idx):
    import json
    path = f"outputs/round_{round_idx}/inner_results.jsonl"
    if not os.path.exists(path):
        print("âš ï¸ æœªæ‰¾åˆ° inner_results.jsonlï¼Œè·³è¿‡ cluster åˆ†æã€‚")
        return
    questions = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                j = json.loads(line)
                questions.append(j.get("question",""))
    ca = ClusterAgent(n_clusters=CFG["default"]["cluster"]["n_clusters"])
    res = ca.analyze_and_suggest(
        questions,
        entropy_threshold=CFG["default"]["cluster"]["entropy_threshold"]
    )
    if res.get("suggestion"):
        print("[ClusterAgent] ğŸ”„ æ›´æ–°ç”Ÿæˆå™¨ promptï¼š", res["suggestion"]["prompt_suggestion"])
        ca.apply_suggestion_to_prompt(res["suggestion"], "synth/prompt_template.txt")
    else:
        print("[ClusterAgent] âœ… é—®é¢˜åˆ†å¸ƒè‰¯å¥½ï¼Œæ— éœ€ä¿®æ”¹ promptã€‚")
    with open(f"outputs/round_{round_idx}/cluster_report.json", "w", encoding="utf-8") as f:
        json.dump(res, f, ensure_ascii=False, indent=2)

# ===== ä¸»æµç¨‹ =====
def main():
    state = load_state()
    total_rounds = CFG["default"]["rounds"]

    # é¦–æ¬¡è¿è¡Œæ—¶ï¼Œéƒ¨ç½²åˆå§‹æ¨¡å‹
    if state["round"] == 0:
        init_model_path = "/data/gaozhitao/modelhub/Qwen3-1.7B"
        restart_vllm_service(init_model_path, port=VLLM_PORT)
        state["current_model"] = f"http::http://localhost:{VLLM_PORT}/generate"
        save_state(state)

    for r in range(state["round"] + 1, total_rounds + 1):
        cur_model = state["current_model"]
        print(f"\n===== ğŸŒ Round {r} å¯åŠ¨ (å½“å‰æ¨¡å‹: {cur_model}) =====")

        # å†…å¾ªç¯
        run_inner_loop(cur_model, r)

        # èšç±»åˆ†æä¸ prompt æ›´æ–°
        cluster_and_update_prompt(r)

        # å¤–å¾ªç¯è®­ç»ƒ
        new_model_local = run_outer_loop(cur_model, r)

        # é‡æ–°éƒ¨ç½² vLLM
        new_model_path = new_model_local.replace("local::", "")
        restart_vllm_service(new_model_path, port=VLLM_PORT)

        # æ›´æ–°çŠ¶æ€
        state["round"] = r
        state["current_model"] = f"http::http://localhost:{VLLM_PORT}/generate"
        state["history"].append({
            "round": r,
            "model": new_model_local,
            "timestamp": datetime.now().isoformat()
        })
        save_state(state)

        print(f"âœ… Round {r} å®Œæˆï¼ŒvLLM å·²æ›´æ–°ä¸ºæ–°æ¨¡å‹ã€‚")
        print("============================================\n")

    print("ğŸ¯ å…¨éƒ¨è½®æ¬¡ PSP è®­ç»ƒå®Œæˆã€‚")

if __name__ == "__main__":
    main()
